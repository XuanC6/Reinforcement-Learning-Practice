import sys
import argparse
import numpy as np
import keras
#import random
import gym


class Imitation():
    def __init__(self, model_config_path, expert_weights_path):
        # Load the expert model.
        with open(model_config_path, 'r') as f:
            self.expert = keras.models.model_from_json(f.read())
        self.expert.load_weights(expert_weights_path)
        
        # Initialize the cloned model (to be trained).
        with open(model_config_path, 'r') as f:
            self.model = keras.models.model_from_json(f.read())

        optimizer = keras.optimizers.Adam(lr = 0.001)
        self.model.compile(loss='categorical_crossentropy',
                           optimizer = optimizer,
                           metrics = ['accuracy'])

    def run_expert(self, env, render=False):
        # Generates an episode by running the expert policy on the given env.
        return Imitation.generate_episode(self.expert, env, render)

    def run_model(self, env, render=False):
        # Generates an episode by running the cloned policy on the given env.
        return Imitation.generate_episode(self.model, env, render)

    @staticmethod
    def generate_episode(model, env, render=False):
        # Generates an episode by running the given model on the given env.
        # Returns:
        # - a list of states, indexed by time step
        # - a list of actions, indexed by time step
        # - a list of rewards, indexed by time step
        states = []
        actions = []
        rewards = []
        
        obs = env.reset()
        if render:
            env.render()
        
        while True:
            states.append(obs)
            # get the softmax output
            action_score = model.predict_on_batch(obs.reshape((-1, 8)))
            # get the selected action index
            action = np.argmax(action_score[0])
            # one-hot encoding
            action_onehot = np.zeros(action_score[0].shape)
            action_onehot[action] = 1
            
            next_obs, reward, done, _ = env.step(action)
            if render:
                env.render()
            rewards.append(reward)
            actions.append(action_onehot)
            obs = next_obs
            
            if done:
                break
        
        return states, actions, rewards

    @staticmethod
    def shuffle_training_data(a, b):
        # shuffle two arrays in the same way
        a = np.array(a)
        b = np.array(b)
        
        assert len(a) == len(b)
        p = np.random.permutation(len(a))
        
        return a[p], b[p]
        

    def train(self, env, num_episodes = 100, num_epochs = 100, render = False):
        # Trains the model on training data generated by the expert policy.
        # Args:
        # - env: The environment to run the expert policy on. 
        # - num_episodes: # episodes to be generated by the expert.
        # - num_epochs: # epochs to train on the data generated by the expert.
        # - render: Whether to render the environment.
        # Returns the final loss and accuracy.
        x_train = []
        y_train = []
        
        for i in range(num_episodes):
            states, actions, _ = self.run_expert(env, render)
            x_train.extend(states)
            y_train.extend(actions)
        
        # shuffle the training data
        x_train, y_train = Imitation.shuffle_training_data(x_train, y_train)
        
        result = self.model.fit(x_train, y_train, batch_size = 32, 
                                epochs = num_epochs, verbose = 0)
        
        final_loss = result.history['loss'][-1]
        final_acc = result.history['acc'][-1]
        
        return final_loss, final_acc
    
    def test(self, env, num_episodes = 50, run_expert = True, render=False):
        # Run expert policy for 50 episodes and record performance
        total_rewards = []
        for i in range(num_episodes):
            if run_expert:
                _, _, rewards = self.run_expert(env, render)
            else:
                _, _, rewards = self.run_model(env, render)
            total_rewards.append(sum(rewards))
            
        total_rewards = np.array(total_rewards)
        reward_mean = np.mean(total_rewards)
        reward_std = np.std(total_rewards)
        
        print('reward mean over 50 episodes:', reward_mean)
        print('reward std over 50 episodes:', reward_std)
        print('')


def parse_arguments():
    # Command-line flags are defined here.
    parser = argparse.ArgumentParser()
    parser.add_argument('--model-config-path', dest='model_config_path',
                        type=str, default='LunarLander-v2-config.json',
                        help="Path to the model config file.")
    parser.add_argument('--expert-weights-path', dest='expert_weights_path',
                        type=str, default='LunarLander-v2-weights.h5',
                        help="Path to the expert weights file.")

    # https://stackoverflow.com/questions/15008758/parsing-boolean-values-with-argparse
    parser_group = parser.add_mutually_exclusive_group(required=False)
    parser_group.add_argument('--render', dest='render',
                              action='store_true',
                              help="Whether to render the environment.")
    parser_group.add_argument('--no-render', dest='render',
                              action='store_false',
                              help="Whether to render the environment.")
    parser.set_defaults(render=False)

    return parser.parse_args()


def main(args):
    # Parse command-line arguments.
    args = parse_arguments()
    model_config_path = args.model_config_path
    expert_weights_path = args.expert_weights_path
    render = args.render
    
    # Create the environment.
    env = gym.make('LunarLander-v2')
    
    # TODO: Train cloned models using imitation learning, and record their
    #       performance.
    dataset_episodes = [1, 10, 50, 100]
    agent = Imitation(model_config_path, expert_weights_path)
    # run expert policy for 50 episodes and record performance
    print('Expert:')
    agent.test(env, run_expert = True)

    # train the model using different number of episodes of data
    # run each model for 50 episodes and record performance
    for num_episodes in dataset_episodes:
        
        final_loss, final_acc = agent.train(env, num_episodes = num_episodes,
                                            render = render)
        
        print('dataset episodes:', num_episodes)
        print('final loss:', final_loss, 'final accuracy:', final_acc)
        agent.test(env, run_expert = False)


if __name__ == '__main__':
    main(sys.argv)
